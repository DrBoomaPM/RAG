{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install python-dotenv\n",
    "%pip install chromadb\n",
    "%pip install pypdf\n",
    "%pip install Flask\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain import OpenAI\n",
    "# from langchain import PromptTemplate\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from flask import Flask\n",
    "# from flask import request\n",
    "# import requests  # Import the requests library\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# prompt_template = \"\"\"\n",
    "# Use the following context to answer the question. \n",
    "# Say \"Sorry I don't know the answer\" if you don't know the answer. \n",
    "# {context}\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# def get_chunks(data):\n",
    "#     splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "#     chunks = splitter.split_documents(data)\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# def get_db_retriever(chunks):\n",
    "#     embeddings = OpenAIEmbeddings()\n",
    "#     db = Chroma.from_documents(chunks, embeddings)\n",
    "\n",
    "#     return db.as_retriever()\n",
    "\n",
    "# def load_data():\n",
    "#     print(\"Loading data from PDF document\")\n",
    "#     pdf_loader = PyPDFLoader(\"./docs/aviva-motor-insurance.pdf\")\n",
    "#     data = pdf_loader.load()\n",
    "\n",
    "#     return data\n",
    "\n",
    "# def retrieval_qa(q):\n",
    "#     llm = OpenAI()\n",
    "#     data = load_data()\n",
    "#     chunks = get_chunks(data)\n",
    "#     db_retriever = get_db_retriever(chunks)\n",
    "\n",
    "#     retrievalQA = RetrievalQA.from_chain_type(\n",
    "#         llm,\n",
    "#         chain_type=\"stuff\",\n",
    "#         retriever=db_retriever,\n",
    "#         chain_type_kwargs={\"prompt\": prompt})\n",
    "\n",
    "#     return retrievalQA({\"query\": q})\n",
    "\n",
    "# def ask(question):\n",
    "#     return retrieval_qa(question)\n",
    "\n",
    "# @app.route(\"/call\", methods=[\"POST\"])\n",
    "# def call():\n",
    "#     req = request.json\n",
    "#     print(\"1. Input is: \", req)\n",
    "#     question = req.get('question')\n",
    "\n",
    "#     response = ask(question)\n",
    "\n",
    "#     return response\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Start Flask app\n",
    "#     app.run(host=\"0.0.0.0\", port=3099, debug=False)\n",
    "\n",
    "#     # Make a test request to the /call endpoint\n",
    "#     url = \"http://127.0.0.1:3099/call\"\n",
    "#     data = {\"question\": \"Is the loss or damage of the vechicle covered? \"}\n",
    "#     response = requests.post(url, json=data)\n",
    "#     print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:3099\n",
      " * Running on http://192.168.0.126:3099\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [03/Dec/2023 01:38:33] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received request: {'question': 'Is the loss or damage of the vehicle is covered?'}\n",
      "Loading data from PDF document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Dec/2023 01:38:43] \"POST /call HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'query': 'Is the loss or damage of the vehicle is covered?', 'result': '\\nNo, the loss or damage of the vehicle is not covered under the Repair Guarantee. The Repair Guarantee only covers damage from deterioration and wear and tear or parts and component failures.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "from dotenv import load_dotenv\n",
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from flask import Flask, send_file\n",
    "from flask import request,abort, render_template, jsonify\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Use the following context to answer the question. \n",
    "\n",
    "Say \"Sorry I don't know the answer\" if you don't know the answer. \n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "def get_chunks(data):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000, chunk_overlap=50)\n",
    "    \n",
    "    chunks = splitter.split_documents(data)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_db_retriever(chunks):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = Chroma.from_documents(chunks,embeddings)\n",
    "\n",
    "    return db.as_retriever()\n",
    "\n",
    "def load_data():\n",
    "    print(\"Loading data from PDF document\")\n",
    "    pdf_loader = PyPDFLoader(\"aviva-motor-insurance.pdf\")\n",
    "    data = pdf_loader.load()\n",
    "\n",
    "    return data\n",
    "\n",
    "def retrieval_qa(q):\n",
    "    \n",
    "    llm = OpenAI()\n",
    "\n",
    "    data = load_data()\n",
    "\n",
    "    chunks = get_chunks(data)\n",
    "\n",
    "    db_retriever = get_db_retriever(chunks)\n",
    "\n",
    "    retrievalQA = RetrievalQA.from_chain_type(llm, \n",
    "        chain_type=\"stuff\", \n",
    "        retriever=db_retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt})\n",
    "\n",
    "    return retrievalQA({\"query\": q})\n",
    "\n",
    "def ask(question):\n",
    "    return retrieval_qa(question)\n",
    "\n",
    "# @app.route(\"/call\",methods=[\"POST\"])\n",
    "# def call():\n",
    "#     req = request.json\n",
    "#     print(\"1. Input is: \",req)\n",
    "#     question = req.get('question')\n",
    "\n",
    "#     response = ask(question)\n",
    "   \n",
    "#     return response\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    try:\n",
    "        return send_file(os.path.join('templates', 'C:/Users/b.poolan.marikannan/.vscode/CPIB/llama2/UI/chat-interface.html'))\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending HTML file: {e}\")\n",
    "        abort(500, description=\"Internal Server Error\")\n",
    "\n",
    "@app.route(\"/call\", methods=[\"POST\"])\n",
    "def call():\n",
    "    try:\n",
    "        req = request.json\n",
    "        print(f\"Received request: {req}\")\n",
    "        question = req.get('question')\n",
    "\n",
    "        response = ask(question)\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "        # Using jsonify to convert the response to JSON\n",
    "        return jsonify({'answer': response})\n",
    "    except Exception as e:\n",
    "        print(f\"Error in call: {e}\")\n",
    "        abort(500, description=\"Internal Server Error\")\n",
    "\n",
    "# def call():\n",
    "#     try:\n",
    "#         req = request.json\n",
    "#         print(f\"Received request: {req}\")\n",
    "#         question = req.get('question')\n",
    "\n",
    "#         response = ask(question)\n",
    "#         print(f\"Response: {response}\")\n",
    "\n",
    "#         return response\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "#         abort(500, description=\"Internal Server Error\")\n",
    "if __name__== '__main__':\n",
    "    app.run(host=\"0.0.0.0\",port=3099)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
